# --- OCR configuration ---
OCR_OUTPUT_DIR=ocr_output
OCR_DPI=200
MIN_TEXT_PER_PAGE=50

# --- Local model configuration (Ollama / llama.cpp) ---
# Base URL of the server (llama.cpp, Ollama, etc.)
# Ollama default: http://localhost:11434
# llama.cpp default: http://localhost:1234
LLAMA_SERVER_URL=http://localhost:1234
OLLAMA_MODEL=llama

# Request timeout in seconds
LLM_TIMEOUT=60

# Max tokens for local model response
LLM_MAX_TOKENS=2048

# Sampling temperature (0.0 = deterministic, higher = more random)
LLM_TEMPERATURE=0.2

# --- Gemini model configuration ---
# Get a free API key at https://aistudio.google.com/apikey
GEMINI_API_KEY=your_key_here
# Model: gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview
GEMINI_MODEL=gemini-3-pro-preview
# Use v1beta for newer models (gemini-3). Omit or false = v1 (stable)
GEMINI_BETA=true
# Max tokens for Gemini response (often needs more than local)
GEMINI_MAX_TOKENS=4096
GEMINI_TEMPERATURE=0.1

# --- Paths ---
# Directory where run summaries are saved (relative to ocr dir or absolute)
# INVOICES_SUMMARY_DIR=invoices_summary

# Optional: path to system prompt file (relative to script dir or absolute)
# PROMPT_FILE=prompt.txt

# --- OCR (scanned PDFs) ---
# Directory for scanned-only PDFs: files here always use OCR (300 DPI). Omit to disable.
# INVOICES_OCR_DIR=invoices_scanned

# DPI when forcing OCR (INVOICES_OCR_DIR or --force-ocr)
# OCR_DPI_FORCED=300

# Tesseract config (--psm 6 = uniform block of text, good for invoices)
# OCR_CONFIG=--psm 6
