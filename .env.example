# --- LLM / Ollama (OpenAI-compatible API) ---
# Base URL of the server (llama.cpp, Ollama, etc.)
# Ollama default: http://localhost:11434
# llama.cpp default: http://localhost:8080
LLAMA_SERVER_URL=http://localhost:8080

# Model name (must match a model available on your server)
OLLAMA_MODEL=llama

# Request timeout in seconds
LLM_TIMEOUT=120

# Max tokens for the model response
LLM_MAX_TOKENS=1024

# Sampling temperature (0.0 = deterministic, higher = more random)
LLM_TEMPERATURE=0.2

# --- Paths ---
# Directory where run summaries are saved (relative to ocr dir or absolute)
# INVOICES_SUMMARY_DIR=invoices_summary

# Optional: path to system prompt file (relative to ocr dir or absolute)
# PROMPT_FILE=prompt.txt

# --- OCR (scanned PDFs) ---
# DPI when rendering PDF pages to images for OCR (higher = better quality, slower)
OCR_DPI=200

# Min characters per page below which OCR fallback is used
MIN_TEXT_PER_PAGE=50
