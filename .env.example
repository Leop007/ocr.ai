# --- OCR configuration ---
OCR_OUTPUT_DIR=ocr_output
OCR_DPI=200
MIN_TEXT_PER_PAGE=50
# Tesseract language (ISO 639-2: eng, fra, deu, etc.). Use + for multiple: eng+fra
OCR_LANG=eng

# --- Local model configuration ---
# llama.cpp (--llm=local): OpenAI-compatible server
LLAMA_SERVER_URL=http://localhost:1234

# Ollama (--llm=ollama): native API
OLLAMA_URL=http://localhost:11434
# Model to use (must exist: ollama pull <model>). Examples: llama, llama3.2, qwen2.5, mistral
OLLAMA_MODEL=llama

# Request timeout in seconds
LLM_TIMEOUT=60

# Max tokens for local model response
LLM_MAX_TOKENS=2048

# Sampling temperature (0.0 = deterministic, higher = more random)
LLM_TEMPERATURE=0.2

# --- Gemini model configuration ---
# Get a free API key at https://aistudio.google.com/apikey
GEMINI_API_KEY=your_key_here
# Model: gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview
GEMINI_MODEL=gemini-3-pro-preview
# Use v1beta for newer models (gemini-3). Omit or false = v1 (stable)
GEMINI_BETA=true
# Max tokens for Gemini response (often needs more than local)
GEMINI_MAX_TOKENS=4096
GEMINI_TEMPERATURE=0.1

# --- Paths ---
# Directory where run summaries are saved (relative to ocr dir or absolute)
# INVOICES_SUMMARY_DIR=invoices_summary

# Run log xlsx (appends each file/run): Source file, Start timestamp, OCR time (s), Summary time (s), Who processed, OCR output, Invoice summary
# INVOICES_RUN_LOG=invoices_run_log.xlsx

# Optional: path to system prompt file (relative to script dir or absolute)
# PROMPT_FILE=prompt.txt

# --- OCR (scanned PDFs) ---
# Directory for scanned-only PDFs: files here always use OCR (300 DPI). Omit to disable.
# INVOICES_OCR_DIR=invoices_scanned

# DPI when forcing OCR (INVOICES_OCR_DIR or --force-ocr)
# OCR_DPI_FORCED=300

# Tesseract config (--psm 6 = uniform block of text, good for invoices)
# OCR_CONFIG=--psm 6
